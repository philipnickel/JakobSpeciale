{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Single Soliton Error and Conservation Analysis\n\nInvestigates:\n\n1. Domain investigation: Fixed N=128, varying L=[20, 30, 40]\n2. Node investigation: Fixed L=30, varying N=[32, 64, 128]\n\nFor each configuration, computes:\n\n- L2 and Linf errors over time\n- Conservation quantities (M, V, E) over time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nfrom concurrent.futures import ProcessPoolExecutor, as_completed\n\nfrom spectral.tdp import KdVSolver, soliton, RK4, RK3\nfrom spectral.utils.norms import discrete_l2_error, discrete_linf_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "DATA_DIR = Path(\"data/A2/ex_d\")\nDATA_DIR.mkdir(parents=True, exist_ok=True)\n\n# Common parameters\nc_vals = [0.25, 0.5, 1.0]\nx0 = 0.0\nT = 20.0\nMETHODS = (\"RK4\", \"RK3\")\nsave_every_steps = 50  # Save every 50 steps\n\n# Domain investigation: varying L, fixed N\nDOMAIN_N = 128\nDOMAIN_L_vals = [20.0, 30.0, 40.0]\n\n# Node investigation: varying N, fixed L\nNODES_L = 30.0\nNODES_N_vals = [32, 64, 128]\n\nmax_workers = os.cpu_count() or 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def estimate_stable_dt(\n    N: int, L: float, method_name: str, c: float, safety_factor=0.1\n) -> float:\n    \"\"\"Estimate stable dt with safety factor.\"\"\"\n    s = KdVSolver(N, L)\n    u_max = float(np.max(np.abs(soliton(s.x, 0.0, c, x0))))\n    dt_est = KdVSolver.stable_dt(N, L, u_max, integrator_name=method_name.lower())\n    dt_safe = safety_factor * dt_est if np.isfinite(dt_est) else 1e-3\n    return float(dt_safe)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def solve_single_case(method: str, N: int, L: float, c: float, investigation_type: str):\n    \"\"\"Solve one case and compute errors + conservation quantities in long format.\"\"\"\n    # Setup integrator\n    integrator_map = {\"RK4\": RK4, \"RK3\": RK3}\n    integ = integrator_map[method]()\n\n    # Setup solver\n    solver = KdVSolver(N, L)\n    x = solver.x\n    dx = solver.dx\n    u0 = soliton(x, 0.0, c, x0)\n\n    # Estimate stable dt\n    dt = estimate_stable_dt(N, L, method, c, safety_factor=0.1)\n\n    # Clear history for multi-step methods\n    if hasattr(integ, \"u_history\"):\n        integ.u_history, integ.f_history = [], []\n\n    # Solve\n    t_saved, u_hist = solver.solve(\n        u0.copy(), T, dt, integrator=integ, save_every=save_every_steps\n    )\n\n    # Compute initial conservation quantities for normalization\n    M0, V0, E0 = KdVSolver.compute_conserved_quantities(u0, dx)\n\n    # Build results in LONG format\n    error_rows = []\n    quantity_rows = []\n\n    for t, u in zip(t_saved, u_hist):\n        u_exact = soliton(x, float(t), c, x0)\n\n        # Compute errors\n        l2_err = discrete_l2_error(u_exact, u, 2 * L)\n        linf_err = discrete_linf_error(u_exact, u)\n\n        # Compute conservation quantities\n        M, V, E = KdVSolver.compute_conserved_quantities(u, dx)\n\n        # Common metadata\n        metadata = {\n            \"t\": t,\n            \"method\": method,\n            \"N\": N,\n            \"L\": L,\n            \"c\": c,\n            \"dt\": dt,\n            \"investigation\": investigation_type,\n        }\n\n        # Store errors in LONG format (one row per error type)\n        for error_type, error_value in [(\"l2\", l2_err), (\"linf\", linf_err)]:\n            error_rows.append(\n                {\n                    **metadata,\n                    \"error_type\": error_type,\n                    \"error\": error_value,\n                }\n            )\n\n        # Store quantities in LONG format (one row per quantity)\n        for qty_name, qty_value, qty_initial in [\n            (\"M\", M, M0),\n            (\"V\", V, V0),\n            (\"E\", E, E0),\n        ]:\n            rel_error = (\n                (qty_value - qty_initial) / qty_initial if qty_initial != 0 else 0.0\n            )\n            quantity_rows.append(\n                {\n                    **metadata,\n                    \"quantity\": qty_name,\n                    \"value\": qty_value,\n                    \"rel_error\": rel_error,\n                }\n            )\n\n    return error_rows, quantity_rows\n\n\nif __name__ == \"__main__\":\n    # %% Build task list\n    print(\"Building task list for exercise d)...\")\n    tasks = []\n\n    # Domain investigation tasks\n    for method in METHODS:\n        for c in c_vals:\n            for L in DOMAIN_L_vals:\n                tasks.append((method, DOMAIN_N, L, c, \"domain\"))\n\n    # Node investigation tasks\n    for method in METHODS:\n        for c in c_vals:\n            for N in NODES_N_vals:\n                tasks.append((method, N, NODES_L, c, \"nodes\"))\n\n    print(f\"Total tasks: {len(tasks)}\")\n    print(\n        f\"  Domain investigation: {len(METHODS) * len(c_vals) * len(DOMAIN_L_vals)} tasks\"\n    )\n    print(\n        f\"  Node investigation: {len(METHODS) * len(c_vals) * len(NODES_N_vals)} tasks\"\n    )\n\n    # %% Run all cases in parallel\n    all_error_rows = []\n    all_quantity_rows = []\n\n    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n        futures = {\n            executor.submit(solve_single_case, m, N, L, c, inv_type): (\n                m,\n                N,\n                L,\n                c,\n                inv_type,\n            )\n            for (m, N, L, c, inv_type) in tasks\n        }\n\n        completed = 0\n        for future in as_completed(futures):\n            m, N, L, c, inv_type = futures[future]\n            try:\n                error_rows, quantity_rows = future.result()\n                all_error_rows.extend(error_rows)\n                all_quantity_rows.extend(quantity_rows)\n                completed += 1\n                if completed % 5 == 0:\n                    print(f\"Completed {completed}/{len(tasks)} tasks...\")\n            except Exception as e:\n                print(\n                    f\"[warn] Task failed: method={m}, N={N}, L={L}, c={c}, type={inv_type} -> {e}\"\n                )\n\n    # %% Create DataFrames and save\n    print(\"\\nCreating DataFrames...\")\n\n    # Error data (already in long format)\n    df_errors = pd.DataFrame(all_error_rows)\n    df_errors[\"method\"] = df_errors[\"method\"].astype(\"category\")\n    df_errors[\"investigation\"] = df_errors[\"investigation\"].astype(\"category\")\n    df_errors[\"error_type\"] = df_errors[\"error_type\"].astype(\"category\")\n\n    # Quantity data (already in long format)\n    df_quantities = pd.DataFrame(all_quantity_rows)\n    df_quantities[\"method\"] = df_quantities[\"method\"].astype(\"category\")\n    df_quantities[\"investigation\"] = df_quantities[\"investigation\"].astype(\"category\")\n    df_quantities[\"quantity\"] = df_quantities[\"quantity\"].astype(\"category\")\n\n    # Save domain investigation data\n    df_domain_errors = df_errors[df_errors[\"investigation\"] == \"domain\"].copy()\n    df_domain_quantities = df_quantities[\n        df_quantities[\"investigation\"] == \"domain\"\n    ].copy()\n\n    out_domain_err = DATA_DIR / \"domain_errors.parquet\"\n    out_domain_qty = DATA_DIR / \"domain_quantities.parquet\"\n    df_domain_errors.to_parquet(out_domain_err, index=False)\n    df_domain_quantities.to_parquet(out_domain_qty, index=False)\n\n    print(\"\\nDomain investigation:\")\n    print(f\"  Saved {len(df_domain_errors):,} error rows \u2192 {out_domain_err}\")\n    print(f\"  Saved {len(df_domain_quantities):,} quantity rows \u2192 {out_domain_qty}\")\n\n    # Save node investigation data\n    df_nodes_errors = df_errors[df_errors[\"investigation\"] == \"nodes\"].copy()\n    df_nodes_quantities = df_quantities[\n        df_quantities[\"investigation\"] == \"nodes\"\n    ].copy()\n\n    out_nodes_err = DATA_DIR / \"nodes_errors.parquet\"\n    out_nodes_qty = DATA_DIR / \"nodes_quantities.parquet\"\n    df_nodes_errors.to_parquet(out_nodes_err, index=False)\n    df_nodes_quantities.to_parquet(out_nodes_qty, index=False)\n\n    print(\"\\nNode investigation:\")\n    print(f\"  Saved {len(df_nodes_errors):,} error rows \u2192 {out_nodes_err}\")\n    print(f\"  Saved {len(df_nodes_quantities):,} quantity rows \u2192 {out_nodes_qty}\")\n\n    print(\"\\nData generation complete!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}